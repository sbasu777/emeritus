{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to PyTorch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbasu777/emeritus/blob/master/Introduction_to_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJc-uAekwDVD",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will introduce PyTorch, talk about its important concepts and features, and eventually train an MNIST classifier using what we have learned. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKJkpn84iMBw",
        "colab_type": "text"
      },
      "source": [
        "## What is PyTorch?\n",
        "\n",
        "1. A Python GPU-accelerated tensor library (NumPy, but faster)\n",
        "2. Differentiable Programming with dynamic computation graphs\n",
        "3. Flexible and efficient **neural network** library\n",
        "4. Python-first framework (easy to integrate with other Python libraries, debug, and extend)\n",
        "  + Quick conversion from & to NumPy array, integration with other Python libs.\n",
        "  + Your favorite Python debugger.\n",
        "  + Adding custom ops with Python/c++ extension. \n",
        "  + Running in purely c++ environment with the c++ API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgmnwpcHf8U0",
        "colab_type": "code",
        "outputId": "839bc94f-050c-42ed-fd98-fbb71b7ab8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# install basical image libs\n",
        "!pip install Pillow>=5.0.0\n",
        "!pip install -U image\n",
        "\n",
        "# install torch and torchvision (a utility library for computer vision that provides many public datasets and pre-trained models)\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.1.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied, skipping upgrade: django in /usr/local/lib/python3.6/dist-packages (from image) (2.2.3)\n",
            "Requirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: sqlparse in /usr/local/lib/python3.6/dist-packages (from django->image) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3oX5WEkM8P",
        "colab_type": "text"
      },
      "source": [
        "## GPU-accelerated Tensor Library\n",
        "\n",
        "A Tensor is a multi-dimensional array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPdEDFjyf94_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cVfJH3PgvCE",
        "colab_type": "code",
        "outputId": "c173e655-8dcb-4aa2-b1ad-40611f6c1b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Create a 3x5 matrix filled with zeros\n",
        "\n",
        "x = torch.zeros(3, 5)\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qauqxYzmgQBO",
        "colab_type": "code",
        "outputId": "adfa4853-ab56-4188-a4ab-4fa807fd776e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Create a 3x5 matrix filled with random values\n",
        "\n",
        "y = torch.randn(3, 5)\n",
        "print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5828,  1.1052,  1.1359, -0.3108,  0.7542],\n",
            "        [-0.1063,  0.7949, -0.0160, -0.3285,  0.0887],\n",
            "        [ 2.1026, -0.8372,  0.8150,  0.7257,  0.4800]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUS9LmmF58gI",
        "colab_type": "code",
        "outputId": "8871ed71-f5d2-4e31-8bfd-9daf712bdc0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Shape manipulations\n",
        "\n",
        "print('\\n.t()  (transpose): ')\n",
        "print(y.t())\n",
        "\n",
        "print('.reshape(5, 3): ')\n",
        "print(y.reshape(5, 3))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ".t()  (transpose): \n",
            "tensor([[-0.5828, -0.1063,  2.1026],\n",
            "        [ 1.1052,  0.7949, -0.8372],\n",
            "        [ 1.1359, -0.0160,  0.8150],\n",
            "        [-0.3108, -0.3285,  0.7257],\n",
            "        [ 0.7542,  0.0887,  0.4800]])\n",
            ".reshape(5, 3): \n",
            "tensor([[-0.5828,  1.1052,  1.1359],\n",
            "        [-0.3108,  0.7542, -0.1063],\n",
            "        [ 0.7949, -0.0160, -0.3285],\n",
            "        [ 0.0887,  2.1026, -0.8372],\n",
            "        [ 0.8150,  0.7257,  0.4800]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-qEF8if6VkQ",
        "colab_type": "code",
        "outputId": "a4061f57-ffbe-4024-c93d-9100c5973561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Slicing\n",
        "\n",
        "print(y[1:])\n",
        "\n",
        "print(y[1:, ::2])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1063,  0.7949, -0.0160, -0.3285,  0.0887],\n",
            "        [ 2.1026, -0.8372,  0.8150,  0.7257,  0.4800]])\n",
            "tensor([[-0.1063, -0.0160,  0.0887],\n",
            "        [ 2.1026,  0.8150,  0.4800]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGpFKCBEgbJe",
        "colab_type": "code",
        "outputId": "52f9baae-1ac4-46e6-b3e8-7d1e9d445bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Basic arithmetics\n",
        "\n",
        "print(x + 2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvoZaG8NkbHx",
        "colab_type": "code",
        "outputId": "fa8bb50d-f03c-4c39-80e8-e4d12cddfe78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(y * (x + 2))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.1656,  2.2105,  2.2717, -0.6215,  1.5084],\n",
            "        [-0.2125,  1.5897, -0.0319, -0.6570,  0.1774],\n",
            "        [ 4.2051, -1.6745,  1.6301,  1.4514,  0.9600]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zktd6b7wkgF2",
        "colab_type": "code",
        "outputId": "2a074c13-4b3c-4ed5-8b82-8db204d5d9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print((y * (x + 2)).exp())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3117,  9.1201,  9.6959,  0.5371,  4.5196],\n",
            "        [ 0.8085,  4.9024,  0.9686,  0.5184,  1.1941],\n",
            "        [67.0280,  0.1874,  5.1043,  4.2693,  2.6117]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2dhiDLzkyQF",
        "colab_type": "text"
      },
      "source": [
        "#### GPU Acceleration\n",
        "\n",
        "Everything can be run on a GPU\n",
        "\n",
        "First, let us create a [`torch.device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device) object representing a GPU device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPdqlb1CkmqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda0 = torch.device('cuda:0')  # pick the GPU at index 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCYhVhc-lE7N",
        "colab_type": "code",
        "outputId": "44faf7a7-ee1e-4a02-ab70-3e4e4d0a69de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Move a tensor from CPU to GPU\n",
        "# NOTE: the first time you access a GPU, a context is created so this may take a\n",
        "# few seconds. But subsequent uses will be fast.\n",
        "\n",
        "cuda_y = y.to(cuda0)\n",
        "print(cuda_y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5828,  1.1052,  1.1359, -0.3108,  0.7542],\n",
            "        [-0.1063,  0.7949, -0.0160, -0.3285,  0.0887],\n",
            "        [ 2.1026, -0.8372,  0.8150,  0.7257,  0.4800]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhhaWnPvlQ0F",
        "colab_type": "code",
        "outputId": "202f552e-1bc3-4b1e-e20c-ca017466e380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Or directly creating a tensor on GPU\n",
        "\n",
        "cuda_x = torch.zeros(3, 5, device=cuda0)\n",
        "print(cuda_x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnaL5mNimjXo",
        "colab_type": "code",
        "outputId": "3b11f963-f442-42e7-a5b8-c5b36b68d4ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# All functions and methods work on GPU tensors\n",
        "\n",
        "print((cuda_y * (cuda_x + 2)).exp())  # values match the CPU results above!"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3117,  9.1201,  9.6959,  0.5371,  4.5196],\n",
            "        [ 0.8085,  4.9024,  0.9686,  0.5184,  1.1941],\n",
            "        [67.0280,  0.1874,  5.1043,  4.2693,  2.6117]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJb9shauw6Bu",
        "colab_type": "text"
      },
      "source": [
        "### NumPy Bridge\n",
        "\n",
        "Converting a `torch.Tensor` to a `np.ndarray` and vice versa is a breeze.\n",
        "\n",
        "The `torch.Tensor` and `np.ndarray` will share their underlying memory locations (if the `torch.Tensor` is on CPU and `dtype` is the same), and changing one will change the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjPmAYWIxeM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho41TWYcw5xt",
        "colab_type": "code",
        "outputId": "512ded1d-df33-4ce7-e8a4-480b71252751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Converting a tensor to an array\n",
        "\n",
        "x = torch.randn(5)\n",
        "print(x)\n",
        "\n",
        "# use `my_tensor.numpy()`\n",
        "x_np = x.numpy()\n",
        "print(x_np)\n",
        "\n",
        "# or `np.asarray`\n",
        "\n",
        "x_np = np.asarray(x)\n",
        "print(x_np)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.7144, -0.5864, -0.0497, -0.3571,  0.6125])\n",
            "[-0.7144131  -0.5864114  -0.04972229 -0.35709804  0.6124573 ]\n",
            "[-0.7144131  -0.5864114  -0.04972229 -0.35709804  0.6124573 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqS8il2AxrAB",
        "colab_type": "code",
        "outputId": "7e54a510-90c9-4522-99fb-b733c28b1e2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# in-place changes on one affects the other\n",
        "\n",
        "x[0] = -1\n",
        "print(x)\n",
        "print(x_np)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.0000, -0.5864, -0.0497, -0.3571,  0.6125])\n",
            "[-1.         -0.5864114  -0.04972229 -0.35709804  0.6124573 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVwkxI8x5ks",
        "colab_type": "code",
        "outputId": "b8f798cb-64c2-4be0-c831-58ff37f47adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Converting an array to a tensor\n",
        "\n",
        "a = np.random.randn(3, 4)\n",
        "\n",
        "a_pt = torch.as_tensor(a)\n",
        "print(a_pt)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.8211, -0.6568, -0.3037, -0.3949],\n",
            "        [-0.8449,  1.1088, -1.2807, -0.6494],\n",
            "        [ 0.6119,  0.7163,  0.1532, -2.3002]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RmnP6Sayb19",
        "colab_type": "code",
        "outputId": "9c32b836-34ea-4c82-c738-9b0ee8e43afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# the resulting CPU Tensor shares memory with the array!\n",
        "\n",
        "a_pt[0] = -1\n",
        "print(a)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.         -1.         -1.         -1.        ]\n",
            " [-0.84490724  1.10883538 -1.28072948 -0.64943817]\n",
            " [ 0.61190272  0.71631575  0.1532047  -2.30022874]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaT4EeJ0yaGh",
        "colab_type": "code",
        "outputId": "cf47c534-7ecb-4d75-995a-cb8f420ca6a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# But if we change dtype and/or device at the same time, a copy is made\n",
        "\n",
        "a_half_pt = torch.as_tensor(a, dtype=torch.float16, device=cuda0)\n",
        "a_half_pt[0] = 9\n",
        "print(a_half_pt)\n",
        "\n",
        "print(a)  # original array is not affected"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 9.0000,  9.0000,  9.0000,  9.0000],\n",
            "        [-0.8447,  1.1084, -1.2803, -0.6494],\n",
            "        [ 0.6118,  0.7163,  0.1532, -2.3008]], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "[[-1.         -1.         -1.         -1.        ]\n",
            " [-0.84490724  1.10883538 -1.28072948 -0.64943817]\n",
            " [ 0.61190272  0.71631575  0.1532047  -2.30022874]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z6UZea7mzza",
        "colab_type": "text"
      },
      "source": [
        "## Differentiable Programming with Dynamic Computation Graphs\n",
        "\n",
        "Gradient-based optimization is an essential part of the modern deep learning frenzy. PyTorch uses [reverse-mode automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) to efficiently compute gradients through any computations done on tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3M1k2eVJTHy",
        "colab_type": "text"
      },
      "source": [
        "### Dynamic vs. Static\n",
        "\n",
        "A neural network is essentially a sequence of mathematical operations on tensors, which build up a computation graph.\n",
        "\n",
        "Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world. One has to build a neural network, and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.\n",
        "\n",
        "PyTorch uses a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GEfWbBdJVAP",
        "colab_type": "text"
      },
      "source": [
        "### Dynamic computation graphs\n",
        "\n",
        "When you create a tensor with its `requires_grad` flag set to `True`, the [`autograd`](https://pytorch.org/docs/stable/autograd.html) engine considers it as a **leaf** node of the computation graph. As you compute with it, the graph is dynamically expanded. When you ask for gradients (e.g., via `tensor.backward()`), the `autograd` engine traces backwards through the graph, and automatically computes the gradients for you.\n",
        "\n",
        "![alt text](https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9oa0qkCJX3T",
        "colab_type": "text"
      },
      "source": [
        "**Let's see this in action!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktc74vsEmq-J",
        "colab_type": "code",
        "outputId": "ac5f09d3-d4f2-472f-ad4c-28c16d07890d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Now, we want tensors with `requires_grad=True`\n",
        "\n",
        "a = torch.ones(3, 5, requires_grad=True)  # tensor of all ones\n",
        "print(a)  # notice that the `requires_grad` flag is on!"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-XhL_zOZhFx",
        "colab_type": "text"
      },
      "source": [
        "Why 1for all elements -  because \n",
        "\n",
        "A = [A11........A15\n",
        "        .....................\n",
        "        A31........A35]\n",
        "        \n",
        " d/dAij (âˆ‘ij Aij) = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3hpPdnrD4v",
        "colab_type": "code",
        "outputId": "0b78f344-2bf4-45b9-8a40-f3af2d25bd62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Currently `a` has no gradients\n",
        "\n",
        "print(a.grad)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muZKbhGWqLAi",
        "colab_type": "code",
        "outputId": "6a6ee88d-f041-44fb-9c4b-4b343e8f8da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's compute the gradient wrt the sum\n",
        "\n",
        "s = a.sum()\n",
        "print('sum of a is', s)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum of a is tensor(15., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N5WmWHpqe_z",
        "colab_type": "code",
        "outputId": "9578ae86-a678-4c86-de0e-511a1f8d78df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Notice the `grad_fn` of `s`. it represents the function used to propagate \n",
        "# gradients from `s` to previous nodes of the graph (`a` in this case).\n",
        "\n",
        "s.backward()  # compute gradient!\n",
        "print(a.grad)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-09eMIzrAvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Yay! Indeed d \\sum_a / d a_ij = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yZUx9OvrPNd",
        "colab_type": "code",
        "outputId": "3e6050d1-2a6b-4018-d7e0-c7171e8367d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Gradients are automatically **accumulated**\n",
        "\n",
        "a.sum().backward()\n",
        "print(a.grad)  # now the new gradients are added to the old ones\n",
        "\n",
        "# Don't worry, we have easy ways to clear the gradients too. \n",
        "# We will talk about those later!"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFVA1MV2rapV",
        "colab_type": "code",
        "outputId": "35af9691-9138-41e7-87f4-d05d0af30c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Now let's do something slightly fancier, on GPU!\n",
        "\n",
        "a = torch.ones(3, 4, device=cuda0, requires_grad=True)\n",
        "b = torch.randn(4, 4, device=cuda0, requires_grad=True)\n",
        "\n",
        "result = (torch.mm(a, b.t().exp()) * 0.5).rfft(2).sum() * b.prod() - b.mean()\n",
        "print('this complicated chain of operation gives....')\n",
        "print(result)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this complicated chain of operation gives....\n",
            "tensor(0.2051, device='cuda:0', grad_fn=<SubBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jP56tn-fQyp",
        "colab_type": "code",
        "outputId": "36f7f4f6-3a7f-44ca-a95f-b6354947edea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "result.backward()\n",
        "print('\\ngradient wrt a is')\n",
        "print(a.grad)\n",
        "print('\\ngradient wrt b is')\n",
        "print(b.grad)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "gradient wrt a is\n",
            "tensor([[-2.6964e-06, -1.5461e-04, -3.5010e-05, -1.5756e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
            "\n",
            "gradient wrt b is\n",
            "tensor([[-0.0621, -0.0632, -0.0623, -0.0660],\n",
            "        [-0.0627, -0.0653, -0.0622, -0.0603],\n",
            "        [-0.0619, -0.0619, -0.0603, -0.0628],\n",
            "        [-0.0644, -0.0633, -0.0622, -0.0622]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IiMxcCYMApV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "499238c0-b315-4ae2-cd2d-52d6311e2db2"
      },
      "source": [
        "#########################\n",
        "#                       #\n",
        "#       Excercise       #\n",
        "#                       #\n",
        "#########################\n",
        "\n",
        "\n",
        "a = torch.linspace(-3, 3, 10, dtype=torch.float32, requires_grad=True)\n",
        "b = torch.logspace(0.2, 2, 10, requires_grad=True)\n",
        "\n",
        "z= torch.log(b.sum() / torch.exp(a).sum()) - b.mean()\n",
        "print(z)\n",
        "\n",
        "z.backward()\n",
        "print(a.grad)\n",
        "print(b.grad)\n",
        "\n",
        "\n",
        "# FIXME!! Compute z as indicated below, and the gradients of z wrt a and b."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-24.9533, grad_fn=<SubBackward0>)\n",
            "tensor([-0.0012, -0.0024, -0.0046, -0.0089, -0.0174, -0.0339, -0.0659, -0.1284,\n",
            "        -0.2501, -0.4872])\n",
            "tensor([-0.0963, -0.0963, -0.0963, -0.0963, -0.0963, -0.0963, -0.0963, -0.0963,\n",
            "        -0.0963, -0.0963])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGlmZlR-NNld",
        "colab_type": "text"
      },
      "source": [
        "Compute \n",
        "\n",
        "$$z = \\log \\left( \\frac{1}{\\sum_i \\exp(a_i)} \\sum_j b_j \\right) - \\frac{1}{\\lvert \\mathbf{b} \\rvert} \\sum_k b_k,$$\n",
        "\n",
        "and then the gradients of $z$ w.r.t. $\\mathbf{a}$ and $\\mathbf{b}$.\n",
        "\n",
        "They should look like:\n",
        "\n",
        "```\n",
        "# Gradient wrt a\n",
        "tensor([-0.0121, -0.0235, -0.0458, -0.0892, -0.1738, -0.3385, -0.6594, -1.2843,\n",
        "        -2.5014, -4.8720])\n",
        "\n",
        "# Gradient wrt b\n",
        "tensor([ 5.3096e-01,  2.9811e-01,  1.5119e-01,  5.8489e-02,  7.4506e-09,\n",
        "        -3.6904e-02, -6.0189e-02, -7.4881e-02, -8.4151e-02, -9.0000e-02])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFNxvwP5u_bs",
        "colab_type": "text"
      },
      "source": [
        "#### Manipulating the `requires_grad` flag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKw8g1TNu5-t",
        "colab_type": "code",
        "outputId": "6ae3904a-abdc-4dd3-f460-e3347eb7cc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Other than directly setting it at creation time, you can change this flag \n",
        "# in-place using `my_tensor.requires_grad_()`, or, as in the above example, or\n",
        "# just directly setting the attribute.\n",
        "\n",
        "x = torch.randn(1, 4, 5)\n",
        "print(x)\n",
        "print('x does not track gradients')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.5907, -1.2147, -1.0973, -0.9304,  0.0787],\n",
            "         [-1.0679, -0.3764,  0.7608, -0.1658,  0.7324],\n",
            "         [-0.6533,  2.1273,  0.1848, -1.7720,  1.2010],\n",
            "         [-0.0467,  0.0066, -0.1344, -0.0926,  1.3753]]])\n",
            "x does not track gradients\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJThUKldvPFN",
        "colab_type": "code",
        "outputId": "a8be01b9-e63b-4d3e-f018-e72a6a44a78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "x.requires_grad_()\n",
        "print(x)\n",
        "print('x now **does** track gradients')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.5907, -1.2147, -1.0973, -0.9304,  0.0787],\n",
            "         [-1.0679, -0.3764,  0.7608, -0.1658,  0.7324],\n",
            "         [-0.6533,  2.1273,  0.1848, -1.7720,  1.2010],\n",
            "         [-0.0467,  0.0066, -0.1344, -0.0926,  1.3753]]], requires_grad=True)\n",
            "x now **does** track gradients\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvixGtnv459",
        "colab_type": "text"
      },
      "source": [
        "## Flexible and Efficient Neural Network Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni-h3bBGz0kf",
        "colab_type": "text"
      },
      "source": [
        "The [`torch.nn`](https://pytorch.org/docs/stable/nn.html) and [`torch.optim`](https://pytorch.org/docs/stable/optim.html) packages provide many efficient implementations of neural network components:\n",
        "  + Affine layers and [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
        "  + Normalization methods\n",
        "  + [Initialization schemes](https://pytorch.org/docs/stable/nn.html#torch-nn-init)\n",
        "  + [Loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "  + [Embeddings](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n",
        "  + [Distributed and Multi-GPU training](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n",
        "  + [Gradient-based optimizers](https://pytorch.org/docs/stable/optim.html)\n",
        "  + [Learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
        "  + etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XWfNgtI0NE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyMj2K002oz8",
        "colab_type": "text"
      },
      "source": [
        "#### `torch.nn` Layers\n",
        "\n",
        "We will use the [fully connected linear layer (`nn.Linear`)](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) as an example. \n",
        "\n",
        "A fc layer performs an affine transform with a 2D weight parameter $\\mathbf{w}$ and a 1D bias parameter $\\mathbf{b}$:\n",
        "\n",
        "$$ f(\\mathbf{x}) = \\mathbf{w}^\\mathrm{T} \\mathbf{x} + \\mathbf{b}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpkSxb3UvZYF",
        "colab_type": "code",
        "outputId": "5de1afa7-67c0-44b4-bcf0-defecb2cfc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fc = nn.Linear(in_features=8, out_features=8)\n",
        "print(fc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=8, out_features=8, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRYaIS5vzyoF",
        "colab_type": "code",
        "outputId": "284d5574-5c15-460b-f61d-9a63592b5617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# It has two parameters, the weight and the bias\n",
        "\n",
        "for name, p in fc.named_parameters():\n",
        "    print('param name: {}\\t shape: {}'.format(name, p.shape))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "param name: weight\t shape: torch.Size([8, 8])\n",
            "param name: bias\t shape: torch.Size([8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMhrp78PNfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgGxaH8P0k6d",
        "colab_type": "code",
        "outputId": "693e47ab-18b2-436b-b07f-d77f05fcd4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# These parameters by default have `requires_grad=True`, so they will collect gradients!\n",
        "\n",
        "print(fc.bias)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([-0.1421, -0.2153,  0.0239,  0.1992, -0.3260,  0.1663,  0.2311,  0.2320],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21nZqKk009u2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's construct an input tensor with 2 dimensions:\n",
        "#   - batch dimension of size 64\n",
        "#   - 8 features\n",
        "\n",
        "x = torch.randn(64, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR0uDfsI1Ybq",
        "colab_type": "code",
        "outputId": "f045028e-b994-4c31-d029-486318b5192e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Pass it through the fc layer\n",
        "\n",
        "result = fc(x)\n",
        "print(result.shape)\n",
        "\n",
        "# Why does the `result` have shape [64, 8]?\n",
        "#   - batch dimension of size 64\n",
        "#   - 8 output features"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKyBTXGE1sQf",
        "colab_type": "code",
        "outputId": "33f13baf-edac-4442-e7a2-9347f650c68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Even though the input `x` has `requires_grad=False`, the convolution\n",
        "# weight and bias parameters has `requires_grad=True`. So the result also\n",
        "# requires gradient, with a `grad_fn` to compute backward pass for \n",
        "# convolutions.\n",
        "print(result.requires_grad)\n",
        "print(result.grad_fn)  # It says `AddmmBackward` because the fc layer performs a matmul and an addition"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "<AddmmBackward object at 0x7f8526386e10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_o_34C-4wdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Say (arbitrarily) we want the layer to behave like the cosine function (yes I know it is impossible)\n",
        "\n",
        "target = x.cos()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBh0R7zV7TZK",
        "colab_type": "code",
        "outputId": "15c379c9-b2db-4f72-b558-cba994e62822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's try MSE loss\n",
        "\n",
        "loss = F.mse_loss(result, target)\n",
        "print(loss)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8142, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HBUX4To7Zie",
        "colab_type": "code",
        "outputId": "615547a2-bd60-43ed-9c01-06cbfb4015ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute gradients\n",
        "\n",
        "loss.backward()\n",
        "print(fc.bias.grad)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1908, -0.2228, -0.1108, -0.1024, -0.2257, -0.1332, -0.0776, -0.1101])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RXQNbrf7jy9",
        "colab_type": "code",
        "outputId": "35f73730-afde-4f79-f4aa-74e4528dd297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# We can manually perform SGD via a loop\n",
        "\n",
        "print('bias before SGD', fc.bias)\n",
        "\n",
        "lr = 0.1\n",
        "with torch.no_grad():  \n",
        "    # this context manager tells PyTorch that we don't want ops inside to be \n",
        "    # tracked by autograd!\n",
        "    for p in fc.parameters():\n",
        "        p -= lr * p.grad\n",
        "        \n",
        "print('bias after SGD', fc.bias)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bias before SGD Parameter containing:\n",
            "tensor([-0.1421, -0.2153,  0.0239,  0.1992, -0.3260,  0.1663,  0.2311,  0.2320],\n",
            "       requires_grad=True)\n",
            "bias after SGD Parameter containing:\n",
            "tensor([-0.1230, -0.1930,  0.0350,  0.2094, -0.3034,  0.1796,  0.2388,  0.2430],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZq4QtJ8F5v4",
        "colab_type": "text"
      },
      "source": [
        "#### `torch.optim` optimizers\n",
        "\n",
        "More easily, we can use the provided [`torch.optim`](https://pytorch.org/docs/stable/optim.html#torch.optim) optimizers. Let's use the [`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer for example!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izmy6Kr3HWEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's optimize for 5000 iterations\n",
        "\n",
        "# First, put the layer on GPU so things run faster\n",
        "\n",
        "fc = fc.to(cuda0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifUdPqu8E-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct an optimizer\n",
        "\n",
        "optim = torch.optim.SGD(fc.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZoO55hiHMrV",
        "colab_type": "code",
        "outputId": "396c5696-654f-4c58-9daa-18c81096bd7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# training loop\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "for ii in range(5000):\n",
        "    # clear gradients accumulated on the parameters\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    # get an input (say we only care inputs sampled from N(0, I))\n",
        "    x = torch.randn(batch_size, 8, device=cuda0)  # this has to be on GPU too\n",
        "    \n",
        "    # target is the cos(x)\n",
        "    target = x.cos()\n",
        "    \n",
        "    # forward pass\n",
        "    result = fc(x)\n",
        "    \n",
        "    # compute loss\n",
        "    loss = F.mse_loss(result, target)\n",
        "    \n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    # let the optimizer do its work; the parameters will be updated in this call\n",
        "    optim.step()\n",
        "    \n",
        "    # add some printing\n",
        "    if ii % 500 == 0:\n",
        "        print('iteration {}\\tloss {:.5f}'.format(ii, loss))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\tloss 0.83702\n",
            "iteration 500\tloss 0.19215\n",
            "iteration 1000\tloss 0.20539\n",
            "iteration 1500\tloss 0.19414\n",
            "iteration 2000\tloss 0.19394\n",
            "iteration 2500\tloss 0.19764\n",
            "iteration 3000\tloss 0.19878\n",
            "iteration 3500\tloss 0.19556\n",
            "iteration 4000\tloss 0.20001\n",
            "iteration 4500\tloss 0.19454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGaMCr8Isrg",
        "colab_type": "text"
      },
      "source": [
        "### Building Deep Neural Neworks\n",
        "\n",
        "A single `nn.Linear` layer didn't do very well! The MSE loss above is still pretty large.\n",
        "\n",
        "But this is expected as it is simply a linear transformation and thus has limited expressive power. Let's replace it with a deep network and see out it works!\n",
        "\n",
        "For simplicity, we will use the following feedforward network architecture (from top to bottom):\n",
        "\n",
        "```\n",
        "        [Input]\n",
        "           ||\n",
        "[Fully-Connected 8 -> 32]\n",
        "           ||\n",
        "    [ReLU activation]\n",
        "           ||\n",
        "[Fully-Connected 32 -> 32]\n",
        "           ||\n",
        "    [ReLU activation]\n",
        "           ||\n",
        "[Fully-Connected 32 -> 8]\n",
        "           ||\n",
        "        [Output]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVzkaIlsLYJT",
        "colab_type": "text"
      },
      "source": [
        "In PyTorch, a model is represented by a [`nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) object. The `nn.Linear` layer we looked at above is also an instance of it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsJ7nLvAHfyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert isinstance(nn.Linear(8, 8), nn.Module)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jztyotM2Lut9",
        "colab_type": "text"
      },
      "source": [
        "Now we want to build a deep network, we can compose the needed layers together by writing a custom `nn.Module` ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny8gXGpfLpNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNet(nn.Module):  # subclass nn.Module\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        \n",
        "        # We need 3 fully-connected layers!\n",
        "        # Simply assigning them as attributes will\n",
        "        # make sure that PyTorch keeps track of them.\n",
        "        \n",
        "        # 8 => 32\n",
        "        self.fc1 = nn.Linear(8, 32)\n",
        "        # 32 => 32\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        # 32 => 8\n",
        "        self.fc3 = nn.Linear(32, 8)\n",
        "        \n",
        "        \n",
        "    # We also need to define a `forward()` method that details\n",
        "    # what should happen when this module is used.\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = x.relu()\n",
        "        x = self.fc2(x)\n",
        "        x = x.relu()\n",
        "        return self.fc3(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuZM4_jeMqsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Okay! Now we are ready to use this deep network! \n",
        "\n",
        "# Construct a network and move to GPU\n",
        "net = MyNet().to(cuda0)\n",
        "\n",
        "# Construct an optimizer\n",
        "optim = torch.optim.SGD(net.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVt8N16MMtxZ",
        "colab_type": "code",
        "outputId": "c52573ad-6fab-4b17-c166-b74fde79b716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# The same training loop, but now using a deep network!\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "for ii in range(5000):\n",
        "    # clear gradients accumulated on the parameters\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    # get an input (say we only care inputs sampled from N(0, I))\n",
        "    x = torch.randn(batch_size, 8, device=cuda0)  # this has to be on GPU too\n",
        "    \n",
        "    # target is the cos(x)\n",
        "    target = x.cos()\n",
        "    \n",
        "    # forward pass\n",
        "    result = net(x)\n",
        "    \n",
        "    # compute loss\n",
        "    loss = F.mse_loss(result, target)\n",
        "    \n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    # let the optimizer do its work; the parameters will be updated in this call\n",
        "    optim.step()\n",
        "    \n",
        "    # add some printing\n",
        "    if ii % 500 == 0:\n",
        "        print('iteration {}\\tloss {:.5f}'.format(ii, loss))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\tloss 0.61165\n",
            "iteration 500\tloss 0.17693\n",
            "iteration 1000\tloss 0.10772\n",
            "iteration 1500\tloss 0.06474\n",
            "iteration 2000\tloss 0.03068\n",
            "iteration 2500\tloss 0.02252\n",
            "iteration 3000\tloss 0.00968\n",
            "iteration 3500\tloss 0.00805\n",
            "iteration 4000\tloss 0.00754\n",
            "iteration 4500\tloss 0.00620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsdcbnOQPbhL",
        "colab_type": "text"
      },
      "source": [
        "The network did so much better than a single fully-connected layer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0b8bbC2Yrrx",
        "colab_type": "text"
      },
      "source": [
        "#### `nn.Module` Containers\n",
        "\n",
        "`torch.nn` also provides many other [`nn.Module` containers](https://pytorch.org/docs/stable/nn.html#containers) for easily building complex networks. E.g., [`nn.Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) executes a list of submodules sequentially, passing each output to the next's input. \n",
        "\n",
        "Using `nn.Sequential`, the above network can be equivalently written as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNUcQt93ZY9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Linear(8, 32),\n",
        "    nn.ReLU(),               # This nn.Module does the ReLU activation on its input\n",
        "    nn.Linear(32, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 8),\n",
        ").to(cuda0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oye4XrPeQR2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########################\n",
        "#                       #\n",
        "#       Excercise       #\n",
        "#                       #\n",
        "#########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySx-KPEcQhmL",
        "colab_type": "text"
      },
      "source": [
        "Perform the same regression task (i.e., modeling $f(x) = \\cos(x)$), but with the following modifications:\n",
        "\n",
        "+ Use one *more* hidden layer\n",
        "+ Use the `tanh` activation function (see [`my_tensor.tanh()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.tanh))\n",
        "+ Use a batch size of 128\n",
        "+ Use the [`torch.optim.Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer\n",
        "+ Use the [L1 loss](https://pytorch.org/docs/stable/nn.html#torch.nn.L1Loss) function\n",
        "\n",
        "\n",
        "The following code skeleton is provided. Fill in the places with `FIXME!!!`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hzs054WQURe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDeeperNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        \n",
        "        # We need 4 fully-connected layers now! \n",
        "        # Each should have 32 output features, except for the last one, which outputs 8 values.\n",
        "        \n",
        "        # FIXME!!!\n",
        "         \n",
        "        # Alternatively, you can use an `nn.Sequential` to implement this.\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # FIXME!!!\n",
        "        # Remember to use the `tanh` activation function\n",
        "        pass\n",
        "        \n",
        "        \n",
        "# Construct our new awesome deeper network and move to GPU\n",
        "deeper_net =  None  # FIXME!!!\n",
        "\n",
        "# Construct an Adam optimizer\n",
        "deeper_net_optim = None  # FIXME!!!\n",
        "\n",
        "\n",
        "# Training loop\n",
        "\n",
        "batch_size = None  # FIXME!!! Batch size of 128\n",
        "\n",
        "for ii in range(5000):\n",
        "    # clear gradients accumulated on the parameters\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    # get an input (say we only care inputs sampled from N(0, I))\n",
        "    x = torch.randn(batch_size, 8, device=cuda0)  # this has to be on GPU too\n",
        "    \n",
        "    # target is the cos(x)\n",
        "    target = x.cos()\n",
        "    \n",
        "    # forward pass\n",
        "    result = deeper_net(x)\n",
        "    \n",
        "    # compute loss\n",
        "    loss = None  # FIXME!!! Now with L1 loss\n",
        "    \n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    # let the optimizer do its work; the parameters will be updated in this call\n",
        "    deeper_net_optim.step()\n",
        "    \n",
        "    # add some printing\n",
        "    if ii % 500 == 0:\n",
        "        print('iteration {}\\tloss {:.5f}'.format(ii, loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}